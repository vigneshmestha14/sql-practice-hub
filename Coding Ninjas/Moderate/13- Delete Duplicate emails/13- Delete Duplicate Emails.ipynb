{"cells":[{"cell_type":"markdown","source":["# **Delete Duplicate Emails**\n","\n","## **Problem Statement**\n","You are given a table named `Person` that may contain **duplicate emails**. Write a **SQL query to delete duplicate email entries**, **keeping only the one with the smallest Id** for each email.\n","\n","---\n","\n","## **Table: Person**\n","\n","| Column Name | Type    |\n","|-------------|---------|\n","| Id          | int     |\n","| Email       | varchar |\n","\n","- `Id` is the primary key of this table.\n","\n","---\n","\n","## **Example**\n","\n","### **Input**\n","\n","| Id | Email            |\n","|----|------------------|\n","| 1  | john@example.com |\n","| 2  | bob@example.com  |\n","| 3  | john@example.com |\n","\n","### **Expected Output (After Deletion)**\n","\n","| Id | Email            |\n","|----|------------------|\n","| 1  | john@example.com |\n","| 2  | bob@example.com  |\n","\n","Only the row with `john@example.com` and the **smallest Id** is kept.\n","\n","---\n","\n","## **Approach 1: PySpark DataFrame API**\n","\n","### **Steps**\n","1. Create a DataFrame with Person data.\n","2. Use `Window` function to rank emails by Id.\n","3. Filter to keep only the **first occurrence** (minimum Id).\n","4. Show the final DataFrame with duplicates removed.\n","\n","### **Code**\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b8e174f4-7113-4da1-aa0e-3196b83fee16"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, col\n","\n","# Step 1: Initialize Spark\n","spark = SparkSession.builder.appName(\"DeleteDuplicateEmails\").getOrCreate()\n","\n","# Step 2: Sample data\n","data = [(1, 'john@example.com'), (2, 'bob@example.com'), (3, 'john@example.com')]\n","columns = [\"Id\", \"Email\"]\n","person_df = spark.createDataFrame(data, columns)\n","\n","# Step 3: Add row number partitioned by email ordered by Id\n","window_spec = Window.partitionBy(\"Email\").orderBy(\"Id\")\n","ranked_df = person_df.withColumn(\"rn\", row_number().over(window_spec))\n","\n","# Step 4: Keep only first occurrences\n","unique_df = ranked_df.filter(col(\"rn\") == 1).drop(\"rn\")\n","\n","# Step 5: Show result\n","unique_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"0a4a5447-27e9-4c5f-8b6a-c5f909c5ba97","normalized_state":"finished","queued_time":"2025-04-09T08:38:27.0360904Z","session_start_time":null,"execution_start_time":"2025-04-09T08:38:27.0373109Z","execution_finish_time":"2025-04-09T08:38:28.6421008Z","parent_msg_id":"dcdb2722-d7ec-4df8-a0c5-53d99f3a8b5d"},"text/plain":"StatementMeta(, 0a4a5447-27e9-4c5f-8b6a-c5f909c5ba97, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+----------------+\n| Id|           Email|\n+---+----------------+\n|  2| bob@example.com|\n|  1|john@example.com|\n+---+----------------+\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb21a9f3-738c-48b8-89db-d448524ff83f"},{"cell_type":"markdown","source":["---\n","\n","## **Approach 2: SQL Query in PySpark**\n","\n","### **Steps**\n","1. Use a **CTE** or subquery to find **minimum Id** for each email.\n","2. Delete from `Person` where Id is **not** in the set of minimum Ids per email.\n","\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8e21d0e-32fc-4f03-8cd2-034c4bf8a624"},{"cell_type":"code","source":["person_df.createOrReplaceTempView(\"Person\")\n","\n","sql_query = \"\"\"\n","    SELECT *\n","    FROM Person\n","    WHERE Id IN (\n","        SELECT MIN(Id)\n","        FROM Person\n","        GROUP BY Email\n","    )\n","\"\"\"\n","\n","result = spark.sql(sql_query)\n","result.show()\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"0a4a5447-27e9-4c5f-8b6a-c5f909c5ba97","normalized_state":"finished","queued_time":"2025-04-09T08:38:27.0733026Z","session_start_time":null,"execution_start_time":"2025-04-09T08:38:28.6442197Z","execution_finish_time":"2025-04-09T08:38:30.131687Z","parent_msg_id":"dc03d28d-4935-4b1b-adb7-fbf925a92ddd"},"text/plain":"StatementMeta(, 0a4a5447-27e9-4c5f-8b6a-c5f909c5ba97, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+----------------+\n| Id|           Email|\n+---+----------------+\n|  1|john@example.com|\n|  2| bob@example.com|\n+---+----------------+\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36742b94-78b8-4281-97c7-4dbdfe5e9f96"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","\n","| Approach         | Method               | Key Technique           |\n","|------------------|----------------------|--------------------------|\n","| **Approach 1**   | PySpark DataFrame API| `Window + row_number()`  |\n","| **Approach 2**   | SQL in PySpark       | `SELECT + GROUP BY`      |\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc93f82c-348a-4223-b004-9c8afa863f8c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}