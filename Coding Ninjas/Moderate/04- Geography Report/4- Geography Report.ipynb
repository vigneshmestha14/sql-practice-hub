{"cells":[{"cell_type":"markdown","source":["# **Geography Report**\n","\n","## **Problem Statement**\n","A U.S. graduate school has students from **Asia, Europe, and America**. The students' **location information** is stored in the table **Student**.\n","\n","### **Table: Student**\n","| Column Name | Type   |\n","|------------|--------|\n","| `name`     | String |\n","| `continent`| String |\n","\n","- Each student belongs to **one continent**.\n","- The number of students in **America** is **greater than or equal** to the number of students in **Asia or Europe**.\n","\n","### **Objective**\n","Pivot the `continent` column so that:\n","- Each `name` is sorted **alphabetically** and displayed under its respective **continent**.\n","- The headers in the output should be **America, Asia, and Europe**.\n","\n","#### **Example**\n","##### **Input Table: Student**\n","| name   | continent |\n","|--------|-----------|\n","| Jack   | America   |\n","| Pascal | Europe    |\n","| Xi     | Asia      |\n","| Jane   | America   |\n","\n","##### **Expected Output**\n","| America | Asia | Europe |\n","|---------|------|--------|\n","| Jack    | Xi   | Pascal |\n","| Jane    |      |        |\n","\n","---\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"588a0c83-3dd7-48d9-b579-2eb7a647e6e3"},{"cell_type":"markdown","source":["## **Approach 1: PySpark DataFrame API**\n","### **Steps**\n","1. **Initialize Spark Session**\n","2. **Create a DataFrame for Student Table**\n","3. **Group by Continent and Collect Names**\n","4. **Sort Names Alphabetically**\n","5. **Use `zip` to Align Names Across Continents**\n","6. **Convert to DataFrame and Display Output**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11cbe567-d11d-44f6-91c2-c37ff4a8979e"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import collect_list, col\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"GeographyReport\").getOrCreate()\n","\n","# Step 2: Create DataFrame for Student Table\n","student_data = [\n","    (\"Jack\", \"America\"),\n","    (\"Pascal\", \"Europe\"),\n","    (\"Xi\", \"Asia\"),\n","    (\"Jane\", \"America\")\n","]\n","student_columns = [\"name\", \"continent\"]\n","\n","student_df = spark.createDataFrame(student_data, student_columns)\n","\n","# Step 3: Group by Continent and Collect Names\n","grouped_df = student_df.groupBy(\"continent\").agg(collect_list(\"name\").alias(\"names\"))\n","\n","# Step 4: Convert to Dictionary and Sort Names\n","grouped_dict = {row[\"continent\"]: sorted(row[\"names\"]) for row in grouped_df.collect()}\n","\n","# Step 5: Align the Names Using Zip\n","max_len = max(len(grouped_dict.get(\"America\", [])), len(grouped_dict.get(\"Asia\", [])), len(grouped_dict.get(\"Europe\", [])))\n","\n","america_list = grouped_dict.get(\"America\", []) + [\"\"] * (max_len - len(grouped_dict.get(\"America\", [])))\n","asia_list = grouped_dict.get(\"Asia\", []) + [\"\"] * (max_len - len(grouped_dict.get(\"Asia\", [])))\n","europe_list = grouped_dict.get(\"Europe\", []) + [\"\"] * (max_len - len(grouped_dict.get(\"Europe\", [])))\n","\n","# Step 6: Convert to DataFrame\n","result_df = spark.createDataFrame(zip(america_list, asia_list, europe_list), [\"America\", \"Asia\", \"Europe\"])\n","\n","# Step 7: Display Output\n","result_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"a7d7575d-3517-4c92-8dd7-bede6eb6092f","normalized_state":"finished","queued_time":"2025-03-30T18:42:15.3517659Z","session_start_time":"2025-03-30T18:42:15.3528831Z","execution_start_time":"2025-03-30T18:42:29.9118781Z","execution_finish_time":"2025-03-30T18:42:34.8679235Z","parent_msg_id":"b6bc8b2f-55ce-43ca-8cd7-009533f42fae"},"text/plain":"StatementMeta(, a7d7575d-3517-4c92-8dd7-bede6eb6092f, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+----+------+\n|America|Asia|Europe|\n+-------+----+------+\n|   Jack|  Xi|Pascal|\n|   Jane|    |      |\n+-------+----+------+\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1c84245-25b0-4214-a384-81053e4afee9"},{"cell_type":"markdown","source":["---\n","\n","## **Approach 2: SQL Query in PySpark**\n","### **Steps**\n","1. **Create a DataFrame for Student Table**\n","2. **Register it as a SQL View**\n","3. **Use ROW_NUMBER() to Assign a Unique Index per Continent**\n","4. **Use Pivoting to Transform Data**\n","5. **Return the Final Table**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d14a620c-f632-4d59-b882-5266ac79dde7"},{"cell_type":"code","source":["# Step 1: Register DataFrame as a SQL View\n","student_df.createOrReplaceTempView(\"Student\")\n","\n","# Step 2: Run SQL Query\n","sql_query = \"\"\"\n","WITH RankedStudents AS (\n","    SELECT \n","        name, \n","        continent,\n","        ROW_NUMBER() OVER (PARTITION BY continent ORDER BY name) AS rn\n","    FROM Student\n",")\n","SELECT \n","    MAX(CASE WHEN continent = 'America' THEN name END) AS America,\n","    MAX(CASE WHEN continent = 'Asia' THEN name END) AS Asia,\n","    MAX(CASE WHEN continent = 'Europe' THEN name END) AS Europe\n","FROM RankedStudents\n","GROUP BY rn;\n","\"\"\"\n","\n","result_sql = spark.sql(sql_query)\n","\n","# Step 3: Display Output\n","result_sql.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"a7d7575d-3517-4c92-8dd7-bede6eb6092f","normalized_state":"finished","queued_time":"2025-03-30T18:42:57.4669802Z","session_start_time":null,"execution_start_time":"2025-03-30T18:42:57.4681737Z","execution_finish_time":"2025-03-30T18:42:58.9286932Z","parent_msg_id":"7a7312db-88a0-4a51-9d9a-e95ca53d2116"},"text/plain":"StatementMeta(, a7d7575d-3517-4c92-8dd7-bede6eb6092f, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+----+------+\n|America|Asia|Europe|\n+-------+----+------+\n|   Jack|  Xi|Pascal|\n|   Jane|NULL|  NULL|\n+-------+----+------+\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"822c2bc6-254b-4441-be7a-33f3a21505d7"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","| Approach  | Method                      | Steps  |\n","|-----------|-----------------------------|--------|\n","| **Approach 1** | PySpark DataFrame API    | Uses `groupBy()`, `collect_list()`, and `zip()` |\n","| **Approach 2** | SQL Query in PySpark     | Uses **ROW_NUMBER() with CASE WHEN for Pivoting** |\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b727ad0f-0112-4b33-9ead-bb9570f38074"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}