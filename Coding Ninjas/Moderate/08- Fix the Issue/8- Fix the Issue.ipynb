{"cells":[{"cell_type":"markdown","source":["# **Fix the Issue**\n","\n","## **Problem Statement**\n","\n","You are given a `Sales` table with the following schema:\n","\n","| Column Name   | Data Type |\n","|---------------|-----------|\n","| sale_id       | INT       |\n","| product_name  | VARCHAR   |\n","| sale_date     | DATE      |\n","\n","- `sale_id` is the primary key.\n","- `product_name` may contain **leading/trailing spaces** and is **case-insensitive**.\n","- You need to:\n","  1. Normalize the `product_name` (trim + lowercase).\n","  2. Format `sale_date` as `'YYYY-MM'`.\n","  3. Count how many times each normalized product was sold per month.\n","  4. Return results sorted by `product_name` and `sale_date`.\n","\n","---\n","\n","#### **Expected Output**\n","\n","| product_name | sale_date | total |\n","|--------------|-----------|-------|\n","| lckeychain   | 2000-02   | 2     |\n","| lcphone      | 2000-01   | 2     |\n","| lcphone      | 2000-02   | 1     |\n","| matryoshka   | 2000-03   | 1     |\n","\n","---\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9f37f355-c564-474d-bd72-bd4852c67a02"},{"cell_type":"markdown","source":["## **Approach 1: PySpark DataFrame API**\n","\n","### **Steps:**\n","1. Clean `product_name` using `trim()` and `lower()`.\n","2. Format `sale_date` using `date_format()`.\n","3. Group by normalized name and formatted date.\n","4. Count rows and sort as required.\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f9a4f01a-3f81-4c15-b281-0181453c72ea"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import trim, lower, date_format, col, count\n","\n","# Step 1: Create Spark session\n","spark = SparkSession.builder.appName(\"FixTheIssue\").getOrCreate()\n","\n","# Step 2: Sample Data\n","data = [\n","    (1, \" LCPHONE\", \"2000-01-16\"),\n","    (2, \"LCPhone \", \"2000-01-17\"),\n","    (3, \" LcPhOnE \", \"2000-02-18\"),\n","    (4, \"LCKeyCHAiN\", \"2000-02-19\"),\n","    (5, \"LCKeyChain\", \"2000-02-28\"),\n","    (6, \"Matryoshka\", \"2000-03-31\")\n","]\n","columns = [\"sale_id\", \"product_name\", \"sale_date\"]\n","\n","df = spark.createDataFrame(data, columns)\n","\n","# Step 3: Clean, transform, and aggregate\n","cleaned_df = df.withColumn(\"product_name\", lower(trim(col(\"product_name\")))) \\\n","               .withColumn(\"sale_date\", date_format(\"sale_date\", \"yyyy-MM\"))\n","\n","result_df = cleaned_df.groupBy(\"product_name\", \"sale_date\") \\\n","                      .agg(count(\"*\").alias(\"total\")) \\\n","                      .orderBy(\"product_name\", \"sale_date\")\n","\n","# Step 4: Show result\n","result_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"9d759812-c3c7-42b1-af85-29a034c410ca","normalized_state":"finished","queued_time":"2025-04-04T17:09:45.801636Z","session_start_time":null,"execution_start_time":"2025-04-04T17:09:45.8028605Z","execution_finish_time":"2025-04-04T17:09:46.5950613Z","parent_msg_id":"ae4b7f18-c700-4cd7-aa65-0bff84569d96"},"text/plain":"StatementMeta(, 9d759812-c3c7-42b1-af85-29a034c410ca, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------------+---------+-----+\n|product_name|sale_date|total|\n+------------+---------+-----+\n|  lckeychain|  2000-02|    2|\n|     lcphone|  2000-01|    2|\n|     lcphone|  2000-02|    1|\n|  matryoshka|  2000-03|    1|\n+------------+---------+-----+\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b88710c-19bb-4325-9011-616653c06cf9"},{"cell_type":"markdown","source":["---\n","\n","## **Approach 2: SQL Query in PySpark**\n","\n","### **Steps:**\n","1. Register the DataFrame as a temp SQL table.\n","2. Write a query to clean `product_name`, format `sale_date`, group and count.\n","3. Use `ORDER BY` on `product_name` and `sale_date`.\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1e90299b-e646-4111-b2c6-eea52f81b9db"},{"cell_type":"code","source":["df.createOrReplaceTempView(\"Sales\")\n","\n","query = \"\"\"\n","SELECT \n","    LOWER(TRIM(product_name)) AS product_name,\n","    DATE_FORMAT(sale_date, 'yyyy-MM') AS sale_date,\n","    COUNT(*) AS total\n","FROM Sales\n","GROUP BY LOWER(TRIM(product_name)), DATE_FORMAT(sale_date, 'yyyy-MM')\n","ORDER BY product_name, sale_date\n","\"\"\"\n","\n","result_sql = spark.sql(query)\n","result_sql.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"9d759812-c3c7-42b1-af85-29a034c410ca","normalized_state":"finished","queued_time":"2025-04-04T17:09:45.8550144Z","session_start_time":null,"execution_start_time":"2025-04-04T17:09:46.5972037Z","execution_finish_time":"2025-04-04T17:09:47.4001463Z","parent_msg_id":"d67daa29-4f81-4e21-8442-23d5cad09824"},"text/plain":"StatementMeta(, 9d759812-c3c7-42b1-af85-29a034c410ca, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------------+---------+-----+\n|product_name|sale_date|total|\n+------------+---------+-----+\n|  lckeychain|  2000-02|    2|\n|     lcphone|  2000-01|    2|\n|     lcphone|  2000-02|    1|\n|  matryoshka|  2000-03|    1|\n+------------+---------+-----+\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f40d10b4-17fb-4d63-8fe4-f48da51478f3"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","| Approach  | Method                      | Key Operations  |\n","|-----------|-----------------------------|----------------|\n","| **Approach 1** | SQL Query in PySpark     | Uses `LOWER(TRIM())`, `DATE_FORMAT()`, `COUNT(*)` |\n","| **Approach 2** | PySpark DataFrame API    | Uses `.withColumn()`, `groupBy().agg()`, `.orderBy()` |"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"93b41aba-b6d1-4a27-8a3d-67688205223e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}