{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8eabcc-e837-4d64-98dc-def6df0e9066",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Warehouse Manager**\n",
    "\n",
    "## **Problem Statement**\n",
    "Given the following tables:\n",
    "\n",
    "### **Table: Warehouse**\n",
    "| Column Name  | Type    |\n",
    "|-------------|---------|\n",
    "| `name`      | varchar |\n",
    "| `product_id`| int     |\n",
    "| `units`     | int     |\n",
    "\n",
    "- **(name, product_id) is the primary key.**\n",
    "- Contains information about **which warehouse stores how many units** of each product.\n",
    "\n",
    "### **Table: Products**\n",
    "| Column Name   | Type    |\n",
    "|--------------|---------|\n",
    "| `product_id` | int     |\n",
    "| `product_name` | varchar |\n",
    "| `Width`      | int     |\n",
    "| `Length`     | int     |\n",
    "| `Height`     | int     |\n",
    "\n",
    "- **product_id is the primary key.**\n",
    "- Contains information about **product dimensions (width, length, height) in feet**.\n",
    "\n",
    "### **Objective**\n",
    "Write an SQL query to **calculate how much cubic feet of volume the inventory occupies in each warehouse**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "### **Input:**\n",
    "\n",
    "#### **Warehouse Table**\n",
    "| name     | product_id | units |\n",
    "|----------|------------|-------|\n",
    "| LCHouse1 | 1          | 1     |\n",
    "| LCHouse1 | 2          | 10    |\n",
    "| LCHouse1 | 3          | 5     |\n",
    "| LCHouse2 | 1          | 2     |\n",
    "| LCHouse2 | 2          | 2     |\n",
    "| LCHouse3 | 4          | 1     |\n",
    "\n",
    "#### **Products Table**\n",
    "| product_id | product_name | Width | Length | Height |\n",
    "|------------|-------------|------|--------|--------|\n",
    "| 1          | LC-TV       | 5    | 50     | 40     |\n",
    "| 2          | LC-KeyChain | 5    | 5      | 5      |\n",
    "| 3          | LC-Phone    | 2    | 10     | 10     |\n",
    "| 4          | LC-T-Shirt  | 4    | 10     | 20     |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77411d8-a392-4ea1-bc03-283e203d067a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **Approach 1: PySpark DataFrame API**\n",
    "### **Steps**\n",
    "1. **Initialize Spark Session**\n",
    "2. **Create DataFrames for Warehouse and Products**\n",
    "3. **Calculate Product Volume (Width × Length × Height)**\n",
    "4. **Join Warehouse with Products on `product_id`**\n",
    "5. **Compute Total Volume for Each Warehouse**\n",
    "6. **Display the Output**\n",
    "### **Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf02f041-1b63-4660-8d8a-aa0f4e366de2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-17T17:35:36.5703989Z",
       "execution_start_time": "2025-03-17T17:35:29.9952465Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "83de4b16-2b3f-406f-ba3a-3a9c4e299cdf",
       "queued_time": "2025-03-17T17:35:12.3894483Z",
       "session_id": "92c4c88f-018e-4157-99d7-204c16cb5a7d",
       "session_start_time": "2025-03-17T17:35:12.390865Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 92c4c88f-018e-4157-99d7-204c16cb5a7d, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    name|volume|\n",
      "+--------+------+\n",
      "|LCHouse2| 20250|\n",
      "|LCHouse3|   800|\n",
      "|LCHouse1| 12250|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, sum\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"WarehouseVolume\").getOrCreate()\n",
    "\n",
    "# Step 2: Create DataFrames\n",
    "warehouse_data = [(\"LCHouse1\", 1, 1), (\"LCHouse1\", 2, 10), (\"LCHouse1\", 3, 5),\n",
    "                  (\"LCHouse2\", 1, 2), (\"LCHouse2\", 2, 2), (\"LCHouse3\", 4, 1)]\n",
    "warehouse_columns = [\"name\", \"product_id\", \"units\"]\n",
    "\n",
    "product_data = [(1, \"LC-TV\", 5, 50, 40), (2, \"LC-KeyChain\", 5, 5, 5), \n",
    "                (3, \"LC-Phone\", 2, 10, 10), (4, \"LC-T-Shirt\", 4, 10, 20)]\n",
    "product_columns = [\"product_id\", \"product_name\", \"Width\", \"Length\", \"Height\"]\n",
    "\n",
    "warehouse_df = spark.createDataFrame(warehouse_data, warehouse_columns)\n",
    "product_df = spark.createDataFrame(product_data, product_columns)\n",
    "\n",
    "# Step 3: Compute Product Volume\n",
    "product_df = product_df.withColumn(\"volume_per_unit\", col(\"Width\") * col(\"Length\") * col(\"Height\"))\n",
    "\n",
    "# Step 4: Join Warehouse and Products on product_id\n",
    "joined_df = warehouse_df.join(product_df, \"product_id\", \"inner\")\n",
    "\n",
    "# Step 5: Compute Total Volume for Each Warehouse\n",
    "result_df = joined_df.withColumn(\"total_volume\", col(\"units\") * col(\"volume_per_unit\")) \\\n",
    "                     .groupBy(\"name\") \\\n",
    "                     .agg(sum(\"total_volume\").alias(\"volume\"))\n",
    "\n",
    "# Step 6: Display Output\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecbe9d-5011-4b1b-810e-87f48fd0d664",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **Approach 2: SQL Query in PySpark**\n",
    "### **Steps**\n",
    "1. **Create Spark Session**\n",
    "2. **Create DataFrames for Warehouse and Products**\n",
    "3. **Register Them as SQL Views**\n",
    "4. **Write and Execute SQL Query**\n",
    "5. **Display the Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a233c5-e85f-4918-b4dc-8945213dd1d9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-17T17:35:38.0528176Z",
       "execution_start_time": "2025-03-17T17:35:36.573008Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a3af870b-a5c8-4c10-a347-1b291ad8382a",
       "queued_time": "2025-03-17T17:35:12.390354Z",
       "session_id": "92c4c88f-018e-4157-99d7-204c16cb5a7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 92c4c88f-018e-4157-99d7-204c16cb5a7d, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|warehouse_name|volume|\n",
      "+--------------+------+\n",
      "|      LCHouse2| 20250|\n",
      "|      LCHouse3|   800|\n",
      "|      LCHouse1| 12250|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"WarehouseSQL\").getOrCreate()\n",
    "\n",
    "# Step 2: Create DataFrames\n",
    "warehouse_df.createOrReplaceTempView(\"Warehouse\")\n",
    "product_df.createOrReplaceTempView(\"Products\")\n",
    "\n",
    "# Step 3: Run SQL Query\n",
    "sql_query = \"\"\"\n",
    "SELECT w.name AS warehouse_name, \n",
    "       SUM(w.units * (p.Width * p.Length * p.Height)) AS volume\n",
    "FROM Warehouse w\n",
    "JOIN Products p ON w.product_id = p.product_id\n",
    "GROUP BY w.name\n",
    "\"\"\"\n",
    "\n",
    "result_sql = spark.sql(sql_query)\n",
    "\n",
    "# Step 4: Display Output\n",
    "result_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96570c03-9e11-476f-bbc3-14c1a21fe6ff",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "| Approach  | Method                      | Steps  |\n",
    "|-----------|-----------------------------|--------|\n",
    "| **Approach 1** | PySpark DataFrame API    | Uses `withColumn()`, `join()`, and `groupBy().agg()` |\n",
    "| **Approach 2** | SQL Query in PySpark     | Uses SQL `JOIN` and `SUM()` |\n",
    "\n",
    "Both approaches return the **same correct result**."
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
