{"cells":[{"cell_type":"markdown","source":["# **Article Views**\n","\n","## **Problem Statement**\n","Given the following table:\n","\n","### **Table: Views**\n","| Column Name | Type  |\n","|------------|-------|\n","| `article_id` | int |\n","| `author_id` | int |\n","| `viewer_id` | int |\n","| `view_date` | date |\n","\n","- The table records **which viewer** viewed **which article** on a given date.\n","- The table **may have duplicate rows**.\n","- **A viewer can also be the author of an article**, but this should not be excluded.\n","- **A person can view multiple articles on the same day.**\n","\n","### **Objective**\n","Write a SQL query to **find all the people (viewer_id) who viewed more than one article on the same date, sorted in ascending order**.\n","\n","---\n","\n","## **Example**\n","\n","### **Input:**\n","\n","#### **Views Table**\n","| article_id | author_id | viewer_id | view_date  |\n","|------------|-----------|-----------|------------|\n","| 1          | 3         | 5         | 2019-08-01 |\n","| 3          | 4         | 5         | 2019-08-01 |\n","| 1          | 3         | 6         | 2019-08-02 |\n","| 2          | 7         | 7         | 2019-08-01 |\n","| 2          | 7         | 6         | 2019-08-02 |\n","| 4          | 7         | 1         | 2019-07-22 |\n","| 3          | 4         | 4         | 2019-07-21 |\n","| 3          | 4         | 4         | 2019-07-21 |\n","\n","---\n","\n","### **Expected Output:**\n","| id |\n","|----|\n","| 5  |\n","| 6  |\n","\n","**Explanation:**\n","- `viewer_id = 5` viewed **two different articles** (`article_id = 1, 3`) on `2019-08-01`.\n","- `viewer_id = 6` viewed **two different articles** (`article_id = 1, 2`) on `2019-08-02`.\n","- `viewer_id = 4` **viewed the same article twice** (`article_id = 3`), but **on the same date**, so they are **not included**.\n","\n","---\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0d832025-2f8d-480d-8f56-1111e39eb677"},{"cell_type":"markdown","source":["## **Approach 1: PySpark DataFrame API**\n","### **Steps**\n","1. **Initialize Spark Session**\n","2. **Create a DataFrame for `Views` Table**\n","3. **Group by `viewer_id` and `view_date`, count distinct articles**\n","4. **Filter viewers who viewed more than 1 distinct article on the same day**\n","5. **Select `viewer_id` and rename as `id`**\n","6. **Sort results in ascending order**\n","7. **Display Output**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"725a77ff-c987-4f66-bb53-f6b19e7c093e"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, countDistinct\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"ArticleViews\").getOrCreate()\n","\n","# Step 2: Create DataFrame for Views Table\n","views_data = [\n","    (1, 3, 5, \"2019-08-01\"),\n","    (3, 4, 5, \"2019-08-01\"),\n","    (1, 3, 6, \"2019-08-02\"),\n","    (2, 7, 7, \"2019-08-01\"),\n","    (2, 7, 6, \"2019-08-02\"),\n","    (4, 7, 1, \"2019-07-22\"),\n","    (3, 4, 4, \"2019-07-21\"),\n","    (3, 4, 4, \"2019-07-21\"),\n","]\n","views_columns = [\"article_id\", \"author_id\", \"viewer_id\", \"view_date\"]\n","\n","views_df = spark.createDataFrame(views_data, views_columns)\n","\n","# Step 3: Group by viewer_id and view_date, count distinct articles\n","grouped_df = views_df.groupBy(\"viewer_id\", \"view_date\").agg(countDistinct(\"article_id\").alias(\"article_count\"))\n","\n","# Step 4: Filter viewers who viewed more than 1 distinct article on the same day\n","filtered_df = grouped_df.filter(col(\"article_count\") > 1)\n","\n","# Step 5: Select viewer_id and rename as id\n","result_df = filtered_df.select(col(\"viewer_id\").alias(\"id\")).distinct()\n","\n","# Step 6: Sort results in ascending order\n","result_df = result_df.orderBy(\"id\")\n","\n","# Step 7: Display Output\n","result_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"submitted","livy_statement_state":"running","session_id":"48862055-b17f-4af0-a3f0-cbd299956e50","normalized_state":"running","queued_time":"2025-03-18T16:02:45.0951941Z","session_start_time":null,"execution_start_time":"2025-03-18T16:02:45.0968176Z","execution_finish_time":null,"parent_msg_id":"6b83d94b-3acd-42ea-9eed-ed6cbc271c67"},"text/plain":"StatementMeta(, 48862055-b17f-4af0-a3f0-cbd299956e50, 5, Submitted, Running, Running)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1be4b456-1e93-4067-a9fa-9ac472e832eb"},{"cell_type":"markdown","source":["\n","---\n","\n","## **Approach 2: SQL Query in PySpark**\n","### **Steps**\n","1. **Create Spark Session**\n","2. **Create DataFrame for Views Table**\n","3. **Register it as a SQL View**\n","4. **Write and Execute SQL Query**\n","5. **Display the Output**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"88682b46-4d62-4198-81bf-ff5d7ac9388f"},{"cell_type":"code","source":["# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"ArticleViewsSQL\").getOrCreate()\n","\n","# Step 2: Register DataFrame as a SQL View\n","views_df.createOrReplaceTempView(\"Views\")\n","\n","# Step 3: Run SQL Query\n","sql_query = \"\"\"\n","SELECT viewer_id AS id\n","FROM Views\n","GROUP BY viewer_id, view_date\n","HAVING COUNT(DISTINCT article_id) > 1\n","ORDER BY viewer_id;\n","\"\"\"\n","\n","result_sql = spark.sql(sql_query)\n","\n","# Step 4: Display Output\n","result_sql.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-18T16:02:45.3089578Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"59400186-243e-4e29-8db8-c5cac3e28ed0"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38977af2-976e-4112-bff0-864af07985ff"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","| Approach  | Method                      | Steps  |\n","|-----------|-----------------------------|--------|\n","| **Approach 1** | PySpark DataFrame API    | Uses `groupBy().agg()`, `countDistinct()`, and `filter()` |\n","| **Approach 2** | SQL Query in PySpark     | Uses SQL `GROUP BY`, `HAVING COUNT(DISTINCT)`, and `ORDER BY` |\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1e6ab13f-1e78-48bd-98ea-26d9fbf78853"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}