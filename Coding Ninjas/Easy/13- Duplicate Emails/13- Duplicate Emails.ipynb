{"cells":[{"cell_type":"markdown","source":["# **Duplicate Emails**\n","\n","## **Problem Statement**\n","You are given a table **Person** with the following structure:\n","\n","### **Table: Person**\n","| Column Name | Type    |\n","|------------|--------|\n","| `Id`       | int    |\n","| `Email`    | varchar |\n","\n","### **Objective**\n","Write a query to return all duplicate emails from the **Person** table.\n","\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"89469bec-a8dc-4e96-b882-77e04b1ebe75"},{"cell_type":"markdown","source":["## **Approach 1: PySpark DataFrame API**\n","### **Steps**\n","1. **Initialize Spark Session**\n","2. **Create a DataFrame for `Person` Table**\n","3. **Group by `Email` and count occurrences**\n","4. **Filter emails that appear more than once**\n","5. **Select and display the output**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"154923b1-e904-4456-89c1-69e1d5138349"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, count\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"DuplicateEmails\").getOrCreate()\n","\n","# Step 2: Create DataFrame for Person Table\n","person_data = [\n","    (1, \"a@b.com\"),\n","    (2, \"c@d.com\"),\n","    (3, \"a@b.com\"),\n","]\n","person_columns = [\"Id\", \"Email\"]\n","\n","person_df = spark.createDataFrame(person_data, person_columns)\n","\n","# Step 3: Group by Email and Count Occurrences\n","duplicate_emails_df = person_df.groupBy(\"Email\").count()\n","\n","# Step 4: Filter Emails with Count > 1\n","duplicate_emails_df = duplicate_emails_df.filter(col(\"count\") > 1)\n","\n","# Step 5: Select Only Email Column\n","result_df = duplicate_emails_df.select(\"Email\")\n","\n","# Step 6: Display Output\n","result_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"11299924-3809-4a59-8769-69d311dc9b6b","normalized_state":"finished","queued_time":"2025-03-20T16:47:18.5230279Z","session_start_time":null,"execution_start_time":"2025-03-20T16:47:18.5245791Z","execution_finish_time":"2025-03-20T16:47:34.4398509Z","parent_msg_id":"870c931f-dcc6-49bc-b93e-b548d3622591"},"text/plain":"StatementMeta(, 11299924-3809-4a59-8769-69d311dc9b6b, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+\n|  Email|\n+-------+\n|a@b.com|\n+-------+\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d80c9a0-5199-4f8e-97b4-4bbca5ee87d8"},{"cell_type":"markdown","source":["---\n","\n","## **Approach 2: SQL Query in PySpark**\n","### **Steps**\n","1. **Create Spark Session**\n","2. **Create DataFrame for `Person` Table**\n","3. **Register it as a SQL View**\n","4. **Write and Execute SQL Query**\n","5. **Display the Output**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"67fc1aea-d08e-4fc0-a582-ed1cc4075ca7"},{"cell_type":"code","source":["# Step 1: Register DataFrame as a SQL View\n","person_df.createOrReplaceTempView(\"Person\")\n","\n","# Step 2: Run SQL Query\n","sql_query = \"\"\"\n","SELECT Email \n","FROM Person\n","GROUP BY Email\n","HAVING COUNT(Email) > 1;\n","\"\"\"\n","\n","result_sql = spark.sql(sql_query)\n","\n","# Step 3: Display Output\n","result_sql.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"11299924-3809-4a59-8769-69d311dc9b6b","normalized_state":"finished","queued_time":"2025-03-20T16:47:18.7277715Z","session_start_time":null,"execution_start_time":"2025-03-20T16:47:34.4427767Z","execution_finish_time":"2025-03-20T16:47:35.2621083Z","parent_msg_id":"5e2475f1-ecf8-4e8d-8057-1589dbb0fb19"},"text/plain":"StatementMeta(, 11299924-3809-4a59-8769-69d311dc9b6b, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+\n|  Email|\n+-------+\n|a@b.com|\n+-------+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4025b5e2-4844-441e-a738-ff568feb45de"},{"cell_type":"markdown","source":["---\n","\n","\n","## **Summary**\n","| Approach  | Method                      | Steps  |\n","|-----------|-----------------------------|--------|\n","| **Approach 1** | PySpark DataFrame API    | Uses `groupBy().count()` and `filter()` |\n","| **Approach 2** | SQL Query in PySpark     | Uses SQL `GROUP BY` and `HAVING COUNT(Email) > 1` |\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5790ff34-5ef0-449c-a386-9c88590f8078"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}