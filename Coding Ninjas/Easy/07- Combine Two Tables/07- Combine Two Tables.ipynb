{"cells":[{"cell_type":"markdown","source":["# **Combine Two Tables**\n","\n","## **Problem Statement**\n","Given two tables:\n","\n","### **Table: Person**\n","| Column Name | Type    |\n","|-------------|--------|\n","| `PersonId`  | int    |\n","| `FirstName` | varchar |\n","| `LastName`  | varchar |\n","\n","- `PersonId` is the **primary key**.\n","- This table contains **personal details**.\n","\n","### **Table: Address**\n","| Column Name | Type    |\n","|-------------|--------|\n","| `AddressId` | int    |\n","| `PersonId`  | int    |\n","| `City`      | varchar |\n","| `State`     | varchar |\n","\n","- `AddressId` is the **primary key**.\n","- `PersonId` is a **foreign key** referencing `Person.PersonId`.\n","- This table contains **address details**.\n","\n","### **Task**\n","Write a **query** to generate a report with:\n","- `FirstName`\n","- `LastName`\n","- `City`\n","- `State`\n","\n","The result should include **all persons**, even if they **don't have an address**.\n","\n","---\n","\n","## **Example**\n","\n","### **Input:**\n","\n","**Person Table**\n","| PersonId | FirstName | LastName |\n","|----------|----------|----------|\n","| 1        | John     | Doe      |\n","| 2        | Jane     | Smith    |\n","| 3        | Alice    | Brown    |\n","\n","**Address Table**\n","| AddressId | PersonId | City    | State  |\n","|-----------|---------|---------|--------|\n","| 1         | 1       | New York| NY     |\n","| 2         | 2       | Boston  | MA     |\n","\n","### **Output:**\n","| FirstName | LastName | City    | State  |\n","|-----------|---------|---------|--------|\n","| John      | Doe     | New York| NY     |\n","| Jane      | Smith   | Boston  | MA     |\n","| Alice     | Brown   | NULL    | NULL   |\n","\n","---\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"00bf078a-1707-45ae-a32d-aeb9fa7b3ce6"},{"cell_type":"markdown","source":["## **Approach 1: PySpark DataFrame API**\n","### **Steps**\n","1. **Initialize Spark Session**  \n","   - Create a Spark session.\n","2. **Load Data**  \n","   - Read `Person.csv` and `Address.csv` into PySpark DataFrames.\n","3. **Perform LEFT JOIN on `PersonId`**  \n","   - Use `join()` function with `\"left\"` join type.\n","4. **Select required columns**  \n","   - Retrieve `FirstName`, `LastName`, `City`, and `State`.\n","5. **Display the Output**  \n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d12eaf18-8e4e-4bbb-a8d9-f5bf64dd629a"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"CombineTwoTables\").getOrCreate()\n","\n","# Step 2: Create Sample Data for Person Table\n","person_data = [\n","    (1, \"John\", \"Doe\"),\n","    (2, \"Jane\", \"Smith\"),\n","    (3, \"Alice\", \"Brown\")\n","]\n","person_columns = [\"PersonId\", \"FirstName\", \"LastName\"]\n","\n","person_df = spark.createDataFrame(person_data, schema=person_columns)\n","\n","# Step 3: Create Sample Data for Address Table\n","address_data = [\n","    (1, 1, \"New York\", \"NY\"),\n","    (2, 2, \"Boston\", \"MA\")\n","]\n","address_columns = [\"AddressId\", \"PersonId\", \"City\", \"State\"]\n","\n","address_df = spark.createDataFrame(address_data, schema=address_columns)\n","\n","# Step 4: Perform LEFT JOIN on PersonId\n","result_df = person_df.join(address_df, \"PersonId\", \"left\").select(\n","    col(\"FirstName\"), col(\"LastName\"), col(\"City\"), col(\"State\")\n",")\n","\n","# Step 5: Display the Output\n","result_df.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"4d6348b5-4b9d-46e6-bede-c26a23bfb498","normalized_state":"finished","queued_time":"2025-03-16T14:52:53.6811339Z","session_start_time":"2025-03-16T14:52:53.6824211Z","execution_start_time":"2025-03-16T14:53:07.4057305Z","execution_finish_time":"2025-03-16T14:53:12.5669337Z","parent_msg_id":"d6d3a2d1-8079-4c0e-a16b-b183ef9b1740"},"text/plain":"StatementMeta(, 4d6348b5-4b9d-46e6-bede-c26a23bfb498, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+--------+--------+-----+\n|FirstName|LastName|    City|State|\n+---------+--------+--------+-----+\n|     John|     Doe|New York|   NY|\n|     Jane|   Smith|  Boston|   MA|\n|    Alice|   Brown|    NULL| NULL|\n+---------+--------+--------+-----+\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f5bc822-218c-4178-9772-01a51f0b20ff"},{"cell_type":"markdown","source":["---\n","\n","## **Approach 2: SQL Query in PySpark**\n","### **Steps**\n","1. **Initialize Spark Session**  \n","   - Create a Spark session.\n","2. **Load Data and Create DataFrames**  \n","   - Read `Person.csv` and `Address.csv` into PySpark DataFrames.\n","3. **Create Temporary SQL Views**  \n","   - Register `Person` and `Address` as **temporary tables**.\n","4. **Write and Execute SQL Query**  \n","   - Perform **LEFT JOIN** on `PersonId` to include all persons.\n","5. **Show Results**  \n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e5725a18-7e47-487a-a094-fc2d90db3986"},{"cell_type":"code","source":["# Step 1-3: Create Temporary Views\n","person_df.createOrReplaceTempView(\"Person\")\n","address_df.createOrReplaceTempView(\"Address\")\n","\n","# Step 4: Run SQL Query\n","query = \"\"\"\n","SELECT p.FirstName, p.LastName, a.City, a.State\n","FROM Person p\n","LEFT JOIN Address a\n","ON p.PersonId = a.PersonId\n","\"\"\"\n","\n","# Step 5: Execute SQL Query\n","sql_result = spark.sql(query)\n","\n","# Step 6: Show Output\n","sql_result.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"4d6348b5-4b9d-46e6-bede-c26a23bfb498","normalized_state":"finished","queued_time":"2025-03-16T14:52:53.6819018Z","session_start_time":null,"execution_start_time":"2025-03-16T14:53:12.5693811Z","execution_finish_time":"2025-03-16T14:53:14.111667Z","parent_msg_id":"a18c99a5-3c54-42e5-86bc-91c8f3aa6d73"},"text/plain":"StatementMeta(, 4d6348b5-4b9d-46e6-bede-c26a23bfb498, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+--------+--------+-----+\n|FirstName|LastName|    City|State|\n+---------+--------+--------+-----+\n|     John|     Doe|New York|   NY|\n|     Jane|   Smith|  Boston|   MA|\n|    Alice|   Brown|    NULL| NULL|\n+---------+--------+--------+-----+\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e00eebbb-af30-4478-a82f-aa76a9948447"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","| Approach | Method | Steps |\n","|----------|--------|-------|\n","| **Approach 1** | PySpark DataFrame API | Uses `join()` with `\"left\"` join type |\n","| **Approach 2** | SQL Query in PySpark | Uses `LEFT JOIN` in SQL |\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"95a5ad52-a8d3-41ea-abcc-5ba5206a44a8"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}