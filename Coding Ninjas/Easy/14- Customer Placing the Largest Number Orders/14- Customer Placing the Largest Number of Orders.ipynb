{"cells":[{"cell_type":"markdown","source":["# **Customer Placing the Largest Number of Orders**\n","\n","## **Problem Statement**\n","You are given a table **Orders** with the following structure:\n","\n","### **Table: Orders**\n","| Column Name     | Type  |\n","|----------------|-------|\n","| `order_number` | int   |\n","| `customer_number` | int |\n","\n","- `order_number` is the primary key.\n","- The table contains information about the order ID and the customer ID.\n","\n","### **Objective**\n","Write a query to find the `customer_number` of the customer who has placed the **largest number of orders**.\n","\n","- It is **guaranteed** that exactly **one customer** has placed more orders than any other customer.\n","\n","---\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d34d1e43-62e6-4618-a98d-f35407a3c3ad"},{"cell_type":"markdown","source":["\n","## **Approach 1: PySpark DataFrame API**\n","### **Steps**\n","1. **Initialize Spark Session**\n","2. **Create a DataFrame for `Orders` Table**\n","3. **Group by `customer_number` and count orders**\n","4. **Find the customer with the maximum order count**\n","5. **Select and display the result**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5202dfc9-7593-4fea-ad76-f2e6f049a6de"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, count, desc\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"LargestNumberOrders\").getOrCreate()\n","\n","# Step 2: Create DataFrame for Orders Table\n","orders_data = [\n","    (1, 1),\n","    (2, 2),\n","    (3, 3),\n","    (4, 3),\n","]\n","orders_columns = [\"order_number\", \"customer_number\"]\n","\n","orders_df = spark.createDataFrame(orders_data, orders_columns)\n","\n","# Step 3: Group by customer_number and Count Orders\n","customer_orders_df = orders_df.groupBy(\"customer_number\").count()\n","\n","# Step 4: Find the Customer with Maximum Order Count\n","max_orders = customer_orders_df.orderBy(desc(\"count\")).limit(1)\n","\n","# Step 5: Select customer_number Column and Display Result\n","result_df = max_orders.select(\"customer_number\")\n","result_df.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"b819523a-cbef-4c0b-8b1b-872db21a9b1b","normalized_state":"finished","queued_time":"2025-03-21T17:42:12.3737432Z","session_start_time":"2025-03-21T17:42:12.3750243Z","execution_start_time":"2025-03-21T17:42:26.596164Z","execution_finish_time":"2025-03-21T17:42:32.0775593Z","parent_msg_id":"d98e5201-4ad7-46fd-a8b0-ca81f79f5230"},"text/plain":"StatementMeta(, b819523a-cbef-4c0b-8b1b-872db21a9b1b, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------------+\n|customer_number|\n+---------------+\n|              3|\n+---------------+\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b9d3dd7b-3802-4366-9f48-750ff60d21e4"},{"cell_type":"markdown","source":["## **Approach 2: SQL Query in PySpark**\n","### **Steps**\n","1. **Create Spark Session**\n","2. **Create DataFrame for `Orders` Table**\n","3. **Register it as a SQL View**\n","4. **Write and Execute SQL Query**\n","5. **Display the Output**\n","\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"917020af-80ef-4ccd-b518-ab81e8c2cee0"},{"cell_type":"code","source":["# Step 1: Register DataFrame as a SQL View\n","orders_df.createOrReplaceTempView(\"Orders\")\n","\n","# Step 2: Run SQL Query\n","sql_query = \"\"\"\n","SELECT customer_number \n","FROM Orders\n","GROUP BY customer_number\n","ORDER BY COUNT(order_number) DESC\n","LIMIT 1;\n","\"\"\"\n","\n","result_sql = spark.sql(sql_query)\n","\n","# Step 3: Display Output\n","result_sql.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"b819523a-cbef-4c0b-8b1b-872db21a9b1b","normalized_state":"finished","queued_time":"2025-03-21T17:42:12.3749811Z","session_start_time":null,"execution_start_time":"2025-03-21T17:42:32.0803113Z","execution_finish_time":"2025-03-21T17:42:33.5667395Z","parent_msg_id":"be4e5bd2-7bda-4051-8b86-e6f91c4db536"},"text/plain":"StatementMeta(, b819523a-cbef-4c0b-8b1b-872db21a9b1b, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------------+\n|customer_number|\n+---------------+\n|              3|\n+---------------+\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1e2e338-247b-4b91-a258-e47e4b6bf59b"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","| Approach  | Method                      | Steps  |\n","|-----------|-----------------------------|--------|\n","| **Approach 1** | PySpark DataFrame API    | Uses `groupBy().count()` and `orderBy().limit(1)` |\n","| **Approach 2** | SQL Query in PySpark     | Uses `GROUP BY`, `ORDER BY COUNT()` and `LIMIT 1` |\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ab476ac-eb37-4a87-b417-956f1dd4db6b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}