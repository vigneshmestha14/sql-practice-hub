{"cells":[{"cell_type":"markdown","source":["# **The Most Frequently Ordered Products for Each Customer**\n","\n","## **Problem Statement**\n","You are given three tables: **Customers, Orders, and Products**.\n","\n","### **Table: Customers**\n","| Column Name   | Type    |\n","|--------------|--------|\n","| `customer_id` | int    |\n","| `name`        | varchar |\n","\n","### **Table: Orders**\n","| Column Name   | Type    |\n","|--------------|--------|\n","| `order_id`    | int    |\n","| `order_date`  | date   |\n","| `customer_id` | int    |\n","| `product_id`  | int    |\n","\n","### **Table: Products**\n","| Column Name   | Type    |\n","|--------------|--------|\n","| `product_id`  | int    |\n","| `product_name` | varchar |\n","| `price`       | int    |\n","\n","### **Objective**\n","Find the most frequently ordered product(s) for each customer who has made at least one order. The result should be sorted by `customer_id`.\n","\n","---\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"974da29c-75d2-4813-815b-e5c4af02f817"},{"cell_type":"markdown","source":["\n","## **Approach 1: PySpark DataFrame API**\n","### **Steps**\n","1. **Initialize Spark Session**\n","2. **Create DataFrames for `Customers`, `Orders`, and `Products`**\n","3. **Aggregate Order Counts for Each (`customer_id`, `product_id`)**\n","4. **Determine the Maximum Order Count per Customer**\n","5. **Join with `Products` Table to Get Product Names**\n","6. **Display the Result**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"15ff8be7-29ec-4b10-a8a2-4d073ac1c194"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, count, max\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"MostFrequentProduct\").getOrCreate()\n","\n","# Step 2: Create DataFrames\n","customers_data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Tom\"), (4, \"Jerry\"), (5, \"John\")]\n","customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\"])\n","\n","orders_data = [\n","    (1, \"2020-07-31\", 1, 1), (2, \"2020-07-30\", 2, 2), (3, \"2020-08-29\", 3, 3),\n","    (4, \"2020-07-29\", 4, 1), (5, \"2020-06-10\", 1, 2), (6, \"2020-08-01\", 2, 1),\n","    (7, \"2020-08-01\", 3, 3), (8, \"2020-08-03\", 1, 2), (9, \"2020-08-07\", 2, 3),\n","    (10, \"2020-07-15\", 1, 2)\n","]\n","orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"order_date\", \"customer_id\", \"product_id\"])\n","\n","products_data = [(1, \"keyboard\", 120), (2, \"mouse\", 80), (3, \"screen\", 600), (4, \"hard disk\", 450)]\n","products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"price\"])\n","\n","# Step 3: Count Orders per (customer_id, product_id)\n","order_counts = orders_df.groupBy(\"customer_id\", \"product_id\").agg(count(\"*\").alias(\"order_count\"))\n","\n","# Step 4: Get Maximum Order Count per Customer\n","max_orders = order_counts.groupBy(\"customer_id\").agg(max(\"order_count\").alias(\"max_count\"))\n","\n","# Step 5: Find Most Frequently Ordered Products per Customer\n","most_frequent = order_counts.join(max_orders, on=\"customer_id\").filter(col(\"order_count\") == col(\"max_count\"))\n","\n","# Step 6: Join with Products Table to Get Product Names\n","result_df = most_frequent.join(products_df, on=\"product_id\").select(\"customer_id\", \"product_id\", \"product_name\").orderBy(\"customer_id\")\n","\n","# Step 7: Display the Result\n","result_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"3f45c77e-48ee-4ddc-aaaa-d166b900c0ff","normalized_state":"finished","queued_time":"2025-03-23T16:48:16.9207403Z","session_start_time":null,"execution_start_time":"2025-03-23T16:48:16.9224017Z","execution_finish_time":"2025-03-23T16:48:18.3982822Z","parent_msg_id":"56bf67c8-4e14-48b8-9820-acc66b4cdc6a"},"text/plain":"StatementMeta(, 3f45c77e-48ee-4ddc-aaaa-d166b900c0ff, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-----------+----------+------------+\n|customer_id|product_id|product_name|\n+-----------+----------+------------+\n|          1|         2|       mouse|\n|          2|         1|    keyboard|\n|          2|         2|       mouse|\n|          2|         3|      screen|\n|          3|         3|      screen|\n|          4|         1|    keyboard|\n+-----------+----------+------------+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2fb830c0-d871-43b6-a390-855e830bfbd6"},{"cell_type":"markdown","source":["\n","---\n","\n","## **Approach 2: SQL Query in PySpark**\n","### **Steps**\n","1. **Create Spark DataFrames and Register them as SQL Views**\n","2. **Write SQL Query to Aggregate and Find Maximum Ordered Product**\n","3. **Execute SQL Query and Display the Output**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"908258fa-6f7c-440d-8741-a7add4c319ad"},{"cell_type":"code","source":["# Step 1: Register DataFrames as SQL Views\n","customers_df.createOrReplaceTempView(\"Customers\")\n","orders_df.createOrReplaceTempView(\"Orders\")\n","products_df.createOrReplaceTempView(\"Products\")\n","\n","# Step 2: Run SQL Query\n","sql_query = \"\"\"\n","WITH OrderCounts AS (\n","    SELECT customer_id, product_id, COUNT(*) AS order_count\n","    FROM Orders\n","    GROUP BY customer_id, product_id\n","),\n","MaxOrders AS (\n","    SELECT customer_id, MAX(order_count) AS max_count\n","    FROM OrderCounts\n","    GROUP BY customer_id\n",")\n","SELECT o.customer_id, o.product_id, p.product_name\n","FROM OrderCounts o\n","JOIN MaxOrders m ON o.customer_id = m.customer_id AND o.order_count = m.max_count\n","JOIN Products p ON o.product_id = p.product_id\n","ORDER BY o.customer_id;\n","\"\"\"\n","\n","result_sql = spark.sql(sql_query)\n","\n","# Step 3: Display Output\n","result_sql.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"3f45c77e-48ee-4ddc-aaaa-d166b900c0ff","normalized_state":"finished","queued_time":"2025-03-23T16:48:16.9889396Z","session_start_time":null,"execution_start_time":"2025-03-23T16:48:18.400856Z","execution_finish_time":"2025-03-23T16:48:20.7407981Z","parent_msg_id":"eed799e7-7480-4b85-9f22-9bb7db6af662"},"text/plain":"StatementMeta(, 3f45c77e-48ee-4ddc-aaaa-d166b900c0ff, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-----------+----------+------------+\n|customer_id|product_id|product_name|\n+-----------+----------+------------+\n|          1|         2|       mouse|\n|          2|         1|    keyboard|\n|          2|         2|       mouse|\n|          2|         3|      screen|\n|          3|         3|      screen|\n|          4|         1|    keyboard|\n+-----------+----------+------------+\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7984310c-da39-4c26-be32-f94fef2b6d67"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","| Approach  | Method                      | Steps  |\n","|-----------|-----------------------------|--------|\n","| **Approach 1** | PySpark DataFrame API    | Uses `groupBy()`, `agg()`, `join()`, and `filter()` |\n","| **Approach 2** | SQL Query in PySpark     | Uses `WITH` CTEs and `JOIN` |\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1d4164c1-98ab-45ec-ba4b-778c1d2e7dea"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}