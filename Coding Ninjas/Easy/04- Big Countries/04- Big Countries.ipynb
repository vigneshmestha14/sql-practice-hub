{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e902c9d6-a22d-4aea-a872-b6265a5c6ea7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Big Countries**\n",
    "## **Problem Statement**\n",
    "There is a table **World** with the following schema:\n",
    "\n",
    "| name         | continent | area     | population | gdp       |\n",
    "|-------------|-----------|----------|------------|-----------|\n",
    "| Afghanistan | Asia      | 652230   | 25500100   | 20343000  |\n",
    "| Albania     | Europe    | 28748    | 2831741    | 12960000  |\n",
    "| Algeria     | Africa    | 2381741  | 37100000   | 188681000 |\n",
    "| Andorra     | Europe    | 468      | 78115      | 3712000   |\n",
    "| Angola      | Africa    | 1246700  | 20609294   | 100990000 |\n",
    "\n",
    "A country is **big** if it has:\n",
    "- An **area** greater than **3,000,000** square km, or\n",
    "- A **population** greater than **25,000,000**.\n",
    "\n",
    "### **Expected Output**\n",
    "| name        | population | area     |\n",
    "|------------|------------|----------|\n",
    "| Afghanistan | 25500100  | 652230   |\n",
    "| Algeria     | 37100000  | 2381741  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f427d-86c0-47e2-a2bc-738e46dadbbc",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## **Approach 1: PySpark DataFrame API**\n",
    "### **Steps**\n",
    "1. **Initialize Spark Session**  \n",
    "   Create a Spark session to process the data.\n",
    "2. **Load Data**  \n",
    "   Load the `World` dataset as a PySpark DataFrame.\n",
    "3. **Apply Filters**  \n",
    "   Use PySpark's `filter()` method to select countries where either:\n",
    "   - The area is greater than **3,000,000** square km.\n",
    "   - The population is greater than **25,000,000**.\n",
    "4. **Select Required Columns**  \n",
    "   Extract only the `name`, `population`, and `area` columns.\n",
    "5. **Show Results**  \n",
    "   Display the output.\n",
    "\n",
    "### **Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a5086d-3fc4-4a88-924c-6e5145b0a9cd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Initialize Spark Session\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"BigCountries\").getOrCreate()\n",
    "\n",
    "# Step 2: Create Data (Simulating World Table)\n",
    "data = [\n",
    "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000),\n",
    "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000),\n",
    "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000),\n",
    "    (\"Andorra\", \"Europe\", 468, 78115, 3712000),\n",
    "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"continent\", \"area\", \"population\", \"gdp\"]\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 4: Apply Filter Conditions\n",
    "filtered_df = df.filter((col(\"area\") > 3000000) | (col(\"population\") > 25000000))\n",
    "\n",
    "# Step 5: Select Required Columns\n",
    "result_df = filtered_df.select(\"name\", \"population\", \"area\")\n",
    "\n",
    "# Step 6: Show Output\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5613f3b-331c-4354-a3be-154f32350c72",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "---\n",
    "## **Approach 2: SQL Query in PySpark**\n",
    "### **Steps**\n",
    "1. **Initialize Spark Session**  \n",
    "   Create a Spark session.\n",
    "2. **Load Data and Create DataFrame**  \n",
    "   Load the dataset into a PySpark DataFrame.\n",
    "3. **Create a Temporary SQL View**  \n",
    "   Register the DataFrame as a SQL table (`world`).\n",
    "4. **Write and Execute SQL Query**  \n",
    "   - Fetch countries where `area > 3,000,000` OR `population > 25,000,000`.\n",
    "   - Select only the `name`, `population`, and `area` columns.\n",
    "5. **Show Results**  \n",
    "   Execute and display the query output.\n",
    "\n",
    "### **Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f26fb-c526-453a-9636-9d089889f87f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-14T12:23:06.8022837Z",
       "execution_start_time": "2025-03-14T12:23:05.3922395Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "84989c21-c580-40a1-a29f-2d4002f142db",
       "queued_time": "2025-03-14T12:22:44.0345652Z",
       "session_id": "356dd1b0-6db5-4a19-9cb2-ccc61e381b7b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 356dd1b0-6db5-4a19-9cb2-ccc61e381b7b, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1-3: Create Temporary View\n",
    "df.createOrReplaceTempView(\"world\")\n",
    "\n",
    "# Step 4: Run SQL Query\n",
    "query = \"\"\"\n",
    "SELECT name, population, area\n",
    "FROM world\n",
    "WHERE area > 3000000 OR population > 25000000\n",
    "\"\"\"\n",
    "\n",
    "# Step 5: Execute SQL Query\n",
    "sql_result = spark.sql(query)\n",
    "\n",
    "# Step 6: Show Output\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53acd980-6ec2-4f39-8a48-1b34852e3f1d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "---\n",
    "## **Summary**\n",
    "| Approach | Method | Steps |\n",
    "|----------|--------|-------|\n",
    "| **Approach 1** | PySpark DataFrame API | Uses `filter()` and `select()` on a DataFrame |\n",
    "| **Approach 2** | SQL Query in PySpark | Uses `createOrReplaceTempView()` and `spark.sql()` |"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
