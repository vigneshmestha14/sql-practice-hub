{"cells":[{"cell_type":"markdown","source":["\n","# **Product's Worth Over Invoices**\n","\n","## **Problem Statement**\n","We are given two tables:\n","\n","### **Table: Product**\n","| Column Name | Type    |\n","|------------|---------|\n","| `product_id`  | int     |\n","| `name`        | varchar |\n","\n","`product_id` is the **primary key**.\n","\n","### **Table: Invoice**\n","| Column Name | Type |\n","|------------|------|\n","| `invoice_id`  | int  |\n","| `product_id`  | int  |\n","| `rest`        | int  |\n","| `paid`        | int  |\n","| `canceled`    | int  |\n","| `refunded`    | int  |\n","\n","`invoice_id` is the **primary key**.\n","\n","### **Objective**\n","For **each product**, calculate the **total amount** of:\n","- `rest` (amount left to pay)\n","- `paid` (amount paid)\n","- `canceled` (amount canceled)\n","- `refunded` (amount refunded)\n","\n","The result should be ordered by `name`.\n","\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0a6f270c-adec-4e55-b595-58dadf467bf2"},{"cell_type":"markdown","source":["\n","## **Approach 1: PySpark DataFrame API**\n","### **Steps**\n","1. **Initialize Spark Session**\n","2. **Create DataFrames for `Product` and `Invoice`**\n","3. **Join both DataFrames on `product_id`**\n","4. **Group by product name and aggregate sum of `rest`, `paid`, `canceled`, and `refunded`**\n","5. **Order by `name`**\n","6. **Display the result**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9028b78f-9196-4e52-b403-e35985cd3889"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import sum, col\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"ProductWorthOverInvoices\").getOrCreate()\n","\n","# Step 2: Create DataFrames for Product and Invoice Tables\n","product_data = [(0, \"ham\"), (1, \"bacon\")]\n","invoice_data = [\n","    (23, 0, 2, 0, 5, 0),\n","    (12, 0, 0, 4, 0, 3),\n","    (1, 1, 1, 1, 0, 1),\n","    (2, 1, 1, 0, 1, 1),\n","    (3, 1, 0, 1, 1, 1),\n","    (4, 1, 1, 1, 1, 0)\n","]\n","\n","product_columns = [\"product_id\", \"name\"]\n","invoice_columns = [\"invoice_id\", \"product_id\", \"rest\", \"paid\", \"canceled\", \"refunded\"]\n","\n","product_df = spark.createDataFrame(product_data, product_columns)\n","invoice_df = spark.createDataFrame(invoice_data, invoice_columns)\n","\n","# Step 3: Join the DataFrames on 'product_id'\n","merged_df = product_df.join(invoice_df, \"product_id\")\n","\n","# Step 4: Aggregate Sum of 'rest', 'paid', 'canceled', 'refunded'\n","result_df = merged_df.groupBy(\"name\").agg(\n","    sum(\"rest\").alias(\"rest\"),\n","    sum(\"paid\").alias(\"paid\"),\n","    sum(\"canceled\").alias(\"canceled\"),\n","    sum(\"refunded\").alias(\"refunded\")\n",")\n","\n","# Step 5: Order by 'name'\n","result_df = result_df.orderBy(\"name\")\n","\n","# Step 6: Display Result\n","result_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"4844201e-3748-4bf8-95b9-95cf0b73d027","normalized_state":"finished","queued_time":"2025-03-26T15:22:07.6143774Z","session_start_time":null,"execution_start_time":"2025-03-26T15:22:07.615564Z","execution_finish_time":"2025-03-26T15:22:09.0914189Z","parent_msg_id":"096f69ce-50a3-4939-96a9-27a0f7ad0f31"},"text/plain":"StatementMeta(, 4844201e-3748-4bf8-95b9-95cf0b73d027, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-----+----+----+--------+--------+\n| name|rest|paid|canceled|refunded|\n+-----+----+----+--------+--------+\n|bacon|   3|   3|       3|       3|\n|  ham|   2|   4|       5|       3|\n+-----+----+----+--------+--------+\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74a76132-0770-4a2a-a061-ece6e526266b"},{"cell_type":"markdown","source":["\n","---\n","\n","## **Approach 2: SQL Query in PySpark**\n","### **Steps**\n","1. **Create DataFrames for `Product` and `Invoice`**\n","2. **Register them as SQL Views**\n","3. **Write and Execute SQL Query**\n","4. **Display the Output**\n","\n","### **Code**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61a147ff-e038-4530-a1d5-54b39178dbd4"},{"cell_type":"code","source":["# Step 1: Register DataFrames as SQL Views\n","product_df.createOrReplaceTempView(\"Product\")\n","invoice_df.createOrReplaceTempView(\"Invoice\")\n","\n","# Step 2: Run SQL Query\n","sql_query = \"\"\"\n","SELECT \n","    p.name,\n","    SUM(i.rest) AS rest,\n","    SUM(i.paid) AS paid,\n","    SUM(i.canceled) AS canceled,\n","    SUM(i.refunded) AS refunded\n","FROM Product p\n","JOIN Invoice i ON p.product_id = i.product_id\n","GROUP BY p.name\n","ORDER BY p.name;\n","\"\"\"\n","\n","result_sql = spark.sql(sql_query)\n","\n","# Step 3: Display Output\n","result_sql.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"4844201e-3748-4bf8-95b9-95cf0b73d027","normalized_state":"finished","queued_time":"2025-03-26T15:22:07.6705258Z","session_start_time":null,"execution_start_time":"2025-03-26T15:22:09.0936247Z","execution_finish_time":"2025-03-26T15:22:10.5663413Z","parent_msg_id":"39b80265-8781-4b42-882a-f12224bbc3c4"},"text/plain":"StatementMeta(, 4844201e-3748-4bf8-95b9-95cf0b73d027, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-----+----+----+--------+--------+\n| name|rest|paid|canceled|refunded|\n+-----+----+----+--------+--------+\n|bacon|   3|   3|       3|       3|\n|  ham|   2|   4|       5|       3|\n+-----+----+----+--------+--------+\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4069e5ff-dca9-4595-8bfe-30ec6d5ef594"},{"cell_type":"markdown","source":["---\n","\n","## **Summary**\n","| Approach  | Method                      | Steps  |\n","|-----------|-----------------------------|--------|\n","| **Approach 1** | PySpark DataFrame API    | Uses `groupBy().agg(sum())` |\n","| **Approach 2** | SQL Query in PySpark     | Uses `SUM()` with `GROUP BY` |"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ab0fa26c-bb3b-4a33-a448-c10541b157de"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}